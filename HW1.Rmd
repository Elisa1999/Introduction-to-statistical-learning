---
title: "HW1"
author: Elisa Zhang
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ISLR2)
```

# Conceptual
## 1. For each of parts (a) through (d), indicate whether we would generally expect the performance of a flexible statistical learning method to be better or worse than an inflexible method. Justify your answer.

## (a) The sample size n is extremely large, and the number of predic- tors p is small.

Better: When the sample size is extremely large, a more flexible model will lead to better performance. Becuase it is easier to detect the statistical pattern with a large sample size in flexible models. 


## (b) The number of predictors p is extremely large, and the number of observations n is small.

Worse: A more flexible model might caused overfitting problem since the number of observations n is small. 


## (c) The relationship between the predictors and response is highly non-linear.

Better: A flexible model will be better as a more restrictive model approximates linear relationship. 

## (d) The variance of the error terms is extremely high.

Worse: We might choose the restrictive model to reduce bias. 


## 2. Explain whether each scenario is a classification or regression problem, and indicate whether we are most interested in inference or prediction. Finally, provide n and p.

## (a) We collect a set of data on the top 500 firms in the US. For each firm we record profit, number of employees, industry and the CEO salary. We are interested in understanding which factors affect CEO salary.

regression, inference. n = 500, p = profit, number of employees, industry. 


## (b) We are considering launching a new product and wish to know whether it will be a success or a failure. We collect data on 20 similar products that were previously launched. For each product we have recorded whether it was a success or failure, price charged for the product, marketing budget, competition price, and ten other variables.

classification, prediction, n = 20, p = price charged for the product, marketing budget, competition price, and ten other variables.

## (c) We are interested in predicting the % change in the USD/Euro exchange rate in relation to the weekly changes in the world stock markets. Hence we collect weekly data for all of 2012. For each week we record the % change in the USD/Euro, the % change in the US market, the % change in the British market, and the % change in the German market.


regression, prediction, n = 52, p = the % change in the US market, the % change in the British market, and the % change in the German market

## 3. We now revisit the bias-variance decomposition.

## (a) Provide a sketch of typical (squared) bias, variance, training error, test error, and Bayes (or irreducible) error curves, on a single plot, as we go from less flexible statistical learning methods towards more flexible approaches. The x-axis should represent the amount of flexibility in the method, and the y-axis should represent the values for each curve. There should be five curves. Make sure to label each one.


## (b) Explain why each of the five curves has the shape displayed in part (a).

The two MSEs have a U-shape as the MSE consist of bias and variance. Bias curve is decreasing dramatically when the flexibility is small. Variance is increasing largely when the flexibility is large. Bayes error curve is a horizontal line since it the gold standard we could not achieve most of times. And the testing error line is above the training error line. 

## 4. You will now think of some real-life applications for statistical learning.

## (a) Describe three real-life applications in which classification might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer.

We want to figure out whether customers will purchase a products based on their personal information.The response variable would be the customer behaviors: purchase or not. The predictors will include the profile of the customers: age, gender, education, postcode etc. The goal could be both. It depends on the scenarios we want. If we want to learn more about who is willing to purchase the products, we might focus on the inference. If we just try to locate the people and send them promotions, we focus on the prediction. 

## (b) Describe three real-life applications in which regression might be useful. Describe the response, as well as the predictors. Is the goal of each application inference or prediction? Explain your answer.

We try to estimate the yearly expense of household living in a certain area. The response variable should be the actual expense of each household. The predictors could include the number of children in the family, the total income of the household, the education of the parents etc. The goal could be both. If we want to learn which kind of people will spend more, the goal would be inference. 

## (c) Describe three real-life applications in which cluster analysis might be useful.

We want to cluster private schools in the Boston to see if they share some common interests. 

## 5. What are the advantages and disadvantages of a very flexible (versus a less flexible) approach for regression or classification? Under what circumstances might a more flexible approach be preferred to a less flexible approach? When might a less flexible approach be preferred?

Ad:a very flexible model might can have higher accuracy. 
Dis: It might bring the overfitting problem. 
A more flexible approach would be preferred when we have a large sample of data and the relationship is highly non-linear. Or we usually prefer a less flexibile model. 

## 6.Describe the differences between a parametric and a non-parametric statistical learning approach. What are the advantages of a para- metric approach to regression or classification (as opposed to a non-parametric approach)? What are its disadvantages?

Parametric method is  a two step and model based. In this method, we make an assumption of the functional form and use that to fit our data. Parametric would be simple to approach the problem with high interpretability. And it need less number of data compared to the non-parametric approach. However, the potential disadvantage is that the model we choose will usually not match the true unknown form of f.


## 7. The table below provides a training data set containing six observations, three predictors, and one qualitative response variable.

## (a) Compute the Euclidean distance between each observation and the test point,X1 =X2 =X3 =0.

$d(ob_1,ob_0) = \sqrt{(3-0)^2 } = 3$
$d(ob_2,ob_0) = \sqrt{(2-0)^2 } = 2$
$d(ob_3,ob_0) = \sqrt{(1-0)^2 + (3-0)^2} = \sqrt{10}$
$d(ob_4,ob_0) = \sqrt{(1-0)^2+ (2-0)^2 } = \sqrt{5}$
$d(ob_5,ob_0) = \sqrt{(0-1)^2 +(1-0)^2 } = \sqrt{2}$
$d(ob_6,ob_0) = \sqrt{(1-0)^2+ (1-0)^2+(1-0)^2 } = \sqrt{3}$

## (b) What is our prediction with K = 1? Why?

when k = 1, our prediction is green. Because Obs.5 is the neartest point to the testing point. 

## (c) What is our prediction with K = 3? Why?

Red. Obs.2, Obs.5 and Obs.6 are the three nearest points and most of them are red.

## (d) If the Bayes decision boundary in this problem is highly non- linear, then would we expect the best value for K to be large or small? Why?

We would expect the best value for K to be larger to grant more flexibility.

## 8. This exercise relates to the College data set, which can be found in the file College.csv on the book website. It contains a number of variables for 777 different universities and colleges in the US. 

## (a)Use the read.csv() function to read the data into R. Call the loaded data college. Make sure that you have the directory set to the correct location for the data.

```{r}
college <- read.csv('College.csv')
```


## (b)Look at the data using the View() function. You should notice that the first column is just the name of each university. We donâ€™t really want R to treat this as data. However, it may be handy to have these names for later. Try the following commands:
```{r}
#assign the row names use the data in the first column
rownames(college) <- college[, 1]
#View(college)
#exliminate the first column
college <- college[, -1]
```

## i. Use the summary() function to produce a numerical summary of the variables in the data set.
```{r}
summary(college)
```

## ii. Use the pairs() function to produce a scatterplot matrix of the first ten columns or variables of the data. Recall that you can reference the first ten columns of a matrix A using A[,1:10].
```{r}
college$Private <- ifelse(college$Private %in% "Yes", TRUE, FALSE)
pairs(college[1:10])
```

## iii. Use the plot() function to produce side-by-side boxplots of Outstate versus Private.

```{r}
Private_y <- college$Outstate[which(college$Private == TRUE)]
Private_n <- college$Outstate[which(college$Private == FALSE)]
boxplot(Private_y ,Private_n, names = c('Yes', 'No'))

```

## iv. Create a new qualitative variable, called Elite, by binning the Top10perc variable. We are going to divide universities into two groups based on whether or not the proportion of students coming from the top 10% of their high school classes exceeds 50 %. Use the summary() function to see how many elite univer- sities there are. Now use the plot() function to produce side-by-side boxplots of Outstate versus Elite.

```{r}
Elite <- rep("No", nrow(college))
Elite[college$Top10perc > 50] <- "Yes" 
Elite <- as.factor(Elite)
college <- data.frame(college, Elite)
summary(college$Elite)
Elite_y <- college$Outstate[which(college$Elite == 'Yes')]
Elite_n <- college$Outstate[which(college$Elite == 'No')]
boxplot(Elite_y ,Elite_n, names = c('Yes', 'No'))
```

## v. Use the hist() function to produce some histograms with differing numbers of bins for a few of the quantitative variables. You may find the command par(mfrow = c(2, 2)) useful: it will divide the print window into four regions so that four plots can be made simultaneously. Modifying the arguments to this function will divide the screen in other ways.
```{r}
par(mfrow = c(2, 2))
hist(college$Enroll)
hist(college$Accept)
hist(college$Top10perc)
hist(college$Top25perc)
```

## vi. Continue exploring the data, and provide a brief summary of what you discover.

```{r}
Private_y <- college$PhD[which(college$Private == TRUE)]
Private_n <- college$PhD[which(college$Private == FALSE)]
boxplot(Private_y ,Private_n, names = c('Yes', 'No'))
```
From the plot above, we can see that, on average, the percent of faculty with Ph.D.â€™s in Public school is higher than that in private school.

## 9. This exercise involves the Auto data set studied in the lab. Make sure that the missing values have been removed from the data.
```{r}
data(Auto)
auto <- na.omit(Auto)
```
## (a) Which of the predictors are quantitative, and which are qualitative?
Only the name of the car is qualitative. The rest of the predictors are quantitative

(b) What is the range of each quantitative predictor? You can answer this using the range() function.
```{r}
for(i in 1:8){
  print(paste('the range of',colnames(auto)[i],'is',range(auto[i])[1],'to', range(auto[i])[2]))
}
```
(c) What is the mean and standard deviation of each quantitative predictor?
```{r}
for(i in 1:8){
  print(paste(colnames(auto)[i],'mean:',apply(auto[i],2,mean),'sd:', apply(auto[i],2,sd)))
}
 
```
(d) Now remove the 10th through 85th observations. What is the range, mean, and standard deviation of each predictor in the subset of the data that remains?
```{r}
auto_rm <- auto[-(10:85),]

for(i in 1:8){
  print(paste(colnames(auto_rm)[i],'mean:',apply(auto_rm[i],2,mean),'sd:', apply(auto_rm[i],2,sd)))
  print(paste('the range of',colnames(auto_rm)[i],'is',range(auto_rm[i])[1],'to', range(auto_rm[i])[2]))
}
```
## (e) Using the full data set, investigate the predictors graphically, using scatterplots or other tools of your choice. Create some plots highlighting the relationships among the predictors. Comment on your findings.
```{r}
plot(auto$year, auto$mpg)
plot(auto$horsepower, auto$mpg)
```

As the year progress, the miles per gallon are increasing. With increasing in horsepower, the miles per gallon have a downward trend.


## (f) Suppose that we wish to predict gas mileage (mpg) on the basis of the other variables. Do your plots suggest that any of the other variables might be useful in predicting mpg? Justify your answer.

Yes, there are some variables are related to the mpg. We could include the year and horsepower as predictors as the plots show above.

## 10. This exercise involves the Boston housing data set.

## (a) To begin, load in the Boston data set. The Boston data set is part of the ISLR2 library.  Now the data set is contained in the object Boston. > Boston Read about the data set: > ?Boston How many rows are in this data set? How many columns? What do the rows and columns represent?
```{r}
data(Boston)
dim(Boston)
colnames(Boston)
```
The columns represent the variables in the data set. And each row is corresponding to one record in each variable. 

## (b) Make some pairwise scatterplots of the predictors (columns) in this data set. Describe your findings.
```{r}
par(mfrow = c(2, 2))
plot(Boston$chas, Boston$crim)
plot(Boston$age, Boston$crim)
plot(Boston$zn, Boston$crim)
plot(Boston$lstat, Boston$crim)
```
Based on the plots above, we can see that the crime rate is positive related to proportion of owner-occupied units built prior to 1940 and lower status of the population. And the crime rate is much lower in the suburbs near the Charles river. 

## (c) Are any of the predictors associated with per capita crime rate? If so, explain the relationship.
Yes, if the proportion of owner-occupied units built prior to 1940 in the suburbs is higher, the rent in that area might be lower. In this way, more poor people might live in that area and is associated with the higher crime rate.

## (d) Do any of the census tracts of Boston appear to have particularly high crime rates? Tax rates? Pupil-teacher ratios? Comment on the range of each predictor.
```{r}
range(Boston$crim)
range(Boston$tax)
range(Boston$ptratio)
```
Yes, there are. There are several tracts having extremely high crime rates, tax rate. The difference in pupil-teacher ratios is not larger when compared to the discrepancy in other two. 

## (e) How many of the census tracts in this data set bound the Charles river?
```{r}
sum(Boston$chas == 1)
```

## (f) What is the median pupil-teacher ratio among the towns in this data set?
```{r}
median(Boston$ptratio)
```

## (g) Which census tract of Boston has lowest median value of owner-occupied homes? What are the values of the other predictors for that census tract, and how do those values compare to the overall ranges for those predictors? Comment on your findings.
```{r}
which(Boston$medv == min(Boston$medv))
Boston[c(399,406),]
```
They have relatively high crime rate per capita and also high proportion of non-retail business acres per town.

## (h) In this data set, how many of the census tracts average more than seven rooms per dwelling? More than eight rooms per dwelling? Comment on the census tracts that average more than eight rooms per dwelling.
```{r}
sum(Boston$rm > 7)
sum(Boston$rm > 8)
Boston[which(Boston$rm > 8),]
```
Those tracts has relatively low crime rate and proportion of owner-occupied units built prior to 1940 is high. 